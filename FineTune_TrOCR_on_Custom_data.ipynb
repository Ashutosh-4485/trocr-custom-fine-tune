{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SdrgsEf08Vgh"
   },
   "source": [
    "## Fine-tune TrOCR on a Custom Dataset\n",
    "\n",
    "This notebook demonstrates how to fine-tune the HuggingFace version of TrOCR using a custom dataset of handwritten text images and transcriptions. It includes optional support for freezing encoder or decoder layers during training.\n",
    "\n",
    "## Set-up environment\n",
    "First, let's install all required libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pkSzlRJq68tH"
   },
   "outputs": [],
   "source": [
    "!pip install -q transformers\n",
    "!pip install -q sentencepiece\n",
    "!pip install -q jiwer\n",
    "!pip install -q datasets\n",
    "!pip install -q evaluate\n",
    "!pip install -q -U accelerate\n",
    "\n",
    "!pip install -q matplotlib\n",
    "!pip install -q protobuf==3.20.1\n",
    "!pip install -q tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nVapjvr9yYMr"
   },
   "source": [
    "##  Upload and extract your dataset (image folder) in Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "whFAuH0KybIr"
   },
   "outputs": [],
   "source": [
    "!unzip \"/content/cropped.zip\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L0kU3VvS8Vgn"
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8LYK528c8Vgn",
    "outputId": "3155f27b-a510-4624-9d75-133bcabd494a"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import evaluate\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob as glob\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from PIL import Image\n",
    "from zipfile import ZipFile\n",
    "from tqdm.notebook import tqdm\n",
    "from dataclasses import dataclass\n",
    "from torch.utils.data import Dataset\n",
    "from urllib.request import urlretrieve\n",
    "from transformers import (\n",
    "    VisionEncoderDecoderModel,\n",
    "    TrOCRProcessor,\n",
    "    Seq2SeqTrainer,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    default_data_collator\n",
    ")\n",
    "\n",
    "block_plot = False\n",
    "plt.rcParams['figure.figsize'] = (12, 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ozD2Es2B5pAh"
   },
   "outputs": [],
   "source": [
    "bold = f\"\\033[1m\"\n",
    "reset = f\"\\033[0m\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HiYvg5Kq8Vgp"
   },
   "outputs": [],
   "source": [
    "def seed_everything(seed_value):\n",
    "    np.random.seed(seed_value)\n",
    "    torch.manual_seed(seed_value)\n",
    "    torch.cuda.manual_seed_all(seed_value)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "seed_everything(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8dcLhOUO8Vgr"
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "72k6fxYLyAZ6"
   },
   "outputs": [],
   "source": [
    "df=pd.read_csv(r\"/content/final_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VgIV5T9JyAZ7"
   },
   "outputs": [],
   "source": [
    "df=df[(df[\"total\"]>=30) & (df[\"total\"]<=50)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZfaQSF7myy2t"
   },
   "source": [
    "### Shuffle and splitting the data in train,test and validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s7s4qk-_yAZ7",
    "outputId": "acdeec31-3540-42e0-a5f3-c3c6392ac8d8"
   },
   "outputs": [],
   "source": [
    "shuffled_df = df.sample(frac=1).reset_index(drop=True)\n",
    "shuffled_df=shuffled_df.iloc[:,:5]\n",
    "shuffled_df.sample(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dp5AweUGyAZ7",
    "outputId": "0bfd1418-368f-4092-f21c-032ba8ad18fa"
   },
   "outputs": [],
   "source": [
    "shuffled_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dc58YF2tyAZ7",
    "outputId": "4175f4b7-5db3-4612-c411-cc6b57259654"
   },
   "outputs": [],
   "source": [
    "train_df=shuffled_df[:part1]\n",
    "test_df=shuffled_df[part1:part2]\n",
    "valid_df=shuffled_df[part2:]\n",
    "train_df.shape,test_df.shape,valid_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mxZc4nva8Vgu"
   },
   "source": [
    "## Training and Dataset Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E6HjTTJD8Vgu"
   },
   "outputs": [],
   "source": [
    "@dataclass(frozen=True)\n",
    "class TrainingConfig:\n",
    "    BATCH_SIZE:    int = 10\n",
    "    EPOCHS:        int = 10\n",
    "    LEARNING_RATE: float = 0.00005\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class DatasetConfig:\n",
    "    DATA_ROOT:     str = 'conten/croppe/cropped//'\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class ModelConfig:\n",
    "    MODEL_NAME: str = 'microsoft/trocr-small-handwritten'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "S3tO7b_7WKAT",
    "outputId": "48a79c48-974c-4a49-aa8f-3d94b08ea044"
   },
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "Ds-O9u_tWKAT",
    "outputId": "5d3950c9-2213-4d94-e22d-0e5128343d81"
   },
   "outputs": [],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OfQtW2jD8Vg0"
   },
   "outputs": [],
   "source": [
    "# Augmentations.\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.ColorJitter(brightness=.5, hue=.3),\n",
    "    transforms.GaussianBlur(kernel_size=(5, 9), sigma=(0.1, 5)),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qO5Q8WYp7DLx"
   },
   "outputs": [],
   "source": [
    "class CustomOCRDataset(Dataset):\n",
    "    def __init__(self, root_dir, df, processor, max_target_length=128):\n",
    "        self.root_dir = root_dir\n",
    "        self.df = df\n",
    "        self.processor = processor\n",
    "        self.max_target_length = max_target_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # The image file name.\n",
    "        file_name = self.df['imscanno'].iloc[idx]\n",
    "        text = self.df['words'].iloc[idx]\n",
    "       # Read the image, apply augmentations, and get the transformed pixels.\n",
    "        image = Image.open(self.root_dir+ str(file_name) + \"_cropped_0.jpg\").convert('RGB')\n",
    "        image = train_transforms(image)\n",
    "        pixel_values = self.processor(image, return_tensors='pt').pixel_values\n",
    "        # Pass the text through the tokenizer and get the labels,\n",
    "        labels = self.processor.tokenizer(\n",
    "            text,\n",
    "            padding='max_length',\n",
    "            max_length=self.max_target_length\n",
    "        ).input_ids\n",
    "        # We are using -100 as the padding token.\n",
    "        labels = [label if label != self.processor.tokenizer.pad_token_id else -100 for label in labels]\n",
    "        encoding = {\"pixel_values\": pixel_values.squeeze(), \"labels\": torch.tensor(labels)}\n",
    "        # print(encoding)\n",
    "        return encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QRlZWXkoyAZ8",
    "outputId": "bdb55076-8f8a-4d17-a409-b8d577782edd"
   },
   "outputs": [],
   "source": [
    "processor = TrOCRProcessor.from_pretrained(ModelConfig.MODEL_NAME)\n",
    "train_dataset = CustomOCRDataset(\n",
    "    root_dir=os.path.join(DatasetConfig.DATA_ROOT),\n",
    "    df=train_df,\n",
    "    processor=processor\n",
    ")\n",
    "valid_dataset = CustomOCRDataset(\n",
    "    root_dir=os.path.join(DatasetConfig.DATA_ROOT),\n",
    "    df=test_df,\n",
    "    processor=processor\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PiwZLbMeLCfo",
    "outputId": "bb017959-f381-4f9e-a2c1-ddd3baca031a"
   },
   "outputs": [],
   "source": [
    "print(\"Number of training examples:\", len(train_dataset))\n",
    "print(\"Number of validation examples:\", len(valid_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rwBNrfD78RA7",
    "outputId": "b2bb85a0-353c-4ba6-a3a2-ace94d733f8f"
   },
   "outputs": [],
   "source": [
    "encoding = train_dataset[0]\n",
    "for k,v in encoding.items():\n",
    "    print(k, v.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FPhXfsY60n-i"
   },
   "source": [
    " ## Dataset Preview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 747
    },
    "id": "QzgOFgD4_7Kw",
    "outputId": "4907c103-1b3d-4d88-a489-3f7b37b83e7c"
   },
   "outputs": [],
   "source": [
    "image = Image.open(train_dataset.root_dir + str(train_df['imscanno'][0]) + \"_cropped_0.jpg\").convert(\"RGB\")\n",
    "image = train_transforms(image)\n",
    "plt.imshow(image)\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vMtfkDia-8tQ",
    "outputId": "81fb7326-2ef5-43a8-c732-e7806834b764"
   },
   "outputs": [],
   "source": [
    "labels = encoding['labels']\n",
    "labels[labels == -100] = processor.tokenizer.pad_token_id\n",
    "label_str = processor.decode(labels, skip_special_tokens=True)\n",
    "print(label_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PYDMgC67zMp6"
   },
   "source": [
    "## TrOCR Architecture Summary\n",
    "1. Encoder: A Vision Transformer (ViT) processes the image and converts it into visual embeddings.\n",
    "\n",
    "2. Decoder: A language model (like GPT-2 or BART-style Transformer) generates text token by token from the encoder output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XxU7TfoYBvg0"
   },
   "source": [
    "## Initialize the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VahrPQKxzQsa"
   },
   "source": [
    "## Strategy 1: Train All Parameters (Encoder + Decoder)\n",
    "#### When to Use:\n",
    "1. Your dataset is very different from the original (e.g., new language, handwriting, new domain).\n",
    "\n",
    "2. You want maximum model flexibility.\n",
    "\n",
    "3. You have enough compute resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "819ca1230e574c91a3f47fbbf551e0d2",
      "1c0b250ec0594d008b0ea6016aa7daac",
      "9636d49865ee4e06890b03aac03ad9a2",
      "17ec8607823f4884b7cbeddb569ef301",
      "4171d68ec7c8406497dc0eb9201109de",
      "73eab676db4a498c8ed8c1bfc231b1ec",
      "bd672655e79a45f4b5c88681e0a73573",
      "6e21f92aebf44e3bb5d1ba78252e292a",
      "a8b9dfcceaf748c2906909db8074d19d",
      "9d0bd1c3f2104ffb87a7b5cae3827bb5",
      "3bce1248c9b74007a8474e466b20f371",
      "316ad79399c8445bb1ac8ae76b2e1a0a",
      "18c95b15743644b9990afa548066ba4d",
      "d0e23da546da42ada17980fe16bb2c8c",
      "ecc3e476985b4cd59ead7470eb888899",
      "1d2ff137c94d4cf38385746ec76ff2dc",
      "bb20ff91d04449f09d1131fff48ec394",
      "3a5bf1f3ac0e4ed28e73d10c00ecb0d2",
      "b0a199bec4ae4644aaed42aee664ecc3",
      "8755bd3e3c6d4326aeea862c311db1ce",
      "c4ad8fd009c74643a2237498099fedb6",
      "5af3ee5c42234d30bf211ec981f140e1",
      "e5e68858cfb24ff5bb743aaca5b4c530",
      "3e905dee5f524e0f9a98692dd16a23d7",
      "16f1827ddf2342f19282122ac868766d",
      "5e56ae6d205548168545937e81a803b5",
      "9e157ab805e34e24bd9f35f8a5edace6",
      "407376985acc4c8997fc950925c0784f",
      "d9c96000951a4e9eaed409c67d6a130f",
      "f4966e0395ae4763b7ed276836709e93",
      "d2764c5346734b288bf1377c692f23a8",
      "3b9b18917f024b7799c3af53647101c3",
      "dea810b298924fd980c18c4e31e48cb8"
     ]
    },
    "id": "bRhvTRrGBIfy",
    "outputId": "54381954-9872-4219-a320-1f4a2cbcf26a"
   },
   "outputs": [],
   "source": [
    "model = VisionEncoderDecoderModel.from_pretrained(ModelConfig.MODEL_NAME)\n",
    "model.to(device)\n",
    "print(model)\n",
    "# Total parameters and trainable parameters.\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"{total_params:,} total parameters.\")\n",
    "total_trainable_params = sum(\n",
    "    p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"{total_trainable_params:,} training parameters.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wbiHuBBMzZcb"
   },
   "source": [
    "## Strategy 2: Freeze Encoder, Train Only Decoder\n",
    "#### When to Use:\n",
    "1. You’re dealing with similar image types (e.g., printed English text) but different output formats.Example: Converting scanned English forms to structured JSON.\n",
    "\n",
    "2. You want the model to learn new language tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PAXdErsqzayr"
   },
   "outputs": [],
   "source": [
    "# # Load the pretrained model\n",
    "# model = VisionEncoderDecoderModel.from_pretrained(ModelConfig.MODEL_NAME)\n",
    "# # model.to(device)\n",
    "\n",
    "# # Freeze the encoder parameters to train only the decoder\n",
    "# for param in model.encoder.parameters():\n",
    "#     param.requires_grad = False\n",
    "\n",
    "# # Print the model to verify which parameters are frozen\n",
    "# print(model)\n",
    "\n",
    "# # Calculate and display total parameters and trainable parameters\n",
    "# total_params = sum(p.numel() for p in model.parameters())\n",
    "# print(f\"{total_params:,} total parameters.\")\n",
    "\n",
    "# total_trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "# print(f\"{total_trainable_params:,} training parameters (decoder only).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ifhmCyoAzdzq"
   },
   "source": [
    "## Strategy 3: Freeze Decoder, Train Only Encoder\n",
    "#### When to Use:\n",
    "1. You’re using new types of images (e.g., handwritten text, new fonts), but the language remains similar.Example: Printed-to-handwritten transfer within the same language.\n",
    "\n",
    "2. You want to preserve language modeling quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BwB9s677zhHq"
   },
   "outputs": [],
   "source": [
    "# # Load the pretrained model\n",
    "# model = VisionEncoderDecoderModel.from_pretrained(ModelConfig.MODEL_NAME)\n",
    "# # model.to(device)  # Uncomment and move the model to GPU or CPU as needed\n",
    "\n",
    "# # Freeze decoder parameters (i.e., do NOT train decoder)\n",
    "# for param in model.decoder.parameters():\n",
    "#     param.requires_grad = False\n",
    "\n",
    "# # (Optional) Freeze the language model head (if you don't want to fine-tune it either)\n",
    "# if hasattr(model, 'lm_head'):\n",
    "#     for param in model.lm_head.parameters():\n",
    "#         param.requires_grad = False\n",
    "\n",
    "# # (Optional) Freeze cross-attention layers in decoder if necessary\n",
    "# if hasattr(model.decoder, \"encoder_attn\"):\n",
    "#     for param in model.decoder.encoder_attn.parameters():\n",
    "#         param.requires_grad = False\n",
    "\n",
    "# # Print model structure (to verify frozen parts)\n",
    "# print(model)\n",
    "\n",
    "# # Count total parameters\n",
    "# total_params = sum(p.numel() for p in model.parameters())\n",
    "# print(f\"{total_params:,} total parameters.\")\n",
    "\n",
    "# # Count only trainable parameters (encoder part)\n",
    "# total_trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "# print(f\"{total_trainable_params:,} training parameters (encoder only).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gCqizZRzzjKj"
   },
   "source": [
    "## Strategy 4: Train Only Last N Layers of Both\n",
    "When to Use:\n",
    "1. You want to fine-tune efficiently with fewer trainable parameters.\n",
    "\n",
    "2. You’re adapting the model slightly (e.g., slight font/layout change or domain shift).\n",
    "\n",
    "3. You want to reduce overfitting or train on limited hardware."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jwCOzVVUzmtS"
   },
   "outputs": [],
   "source": [
    "# # Load pretrained TrOCR model\n",
    "# model = VisionEncoderDecoderModel.from_pretrained(ModelConfig.MODEL_NAME)\n",
    "\n",
    "# # Freeze all parameters\n",
    "# for param in model.parameters():\n",
    "#     param.requires_grad = False\n",
    "\n",
    "# # Unfreeze last N layers of encoder\n",
    "# N = 4  # Number of last layers to train\n",
    "\n",
    "# # Check and unfreeze encoder layers\n",
    "# if hasattr(model.encoder, \"encoder\") and hasattr(model.encoder.encoder, \"layer\"):\n",
    "#     encoder_layers = model.encoder.encoder.layer\n",
    "#     for layer in encoder_layers[-N:]:\n",
    "#         for param in layer.parameters():\n",
    "#             param.requires_grad = True\n",
    "# else:\n",
    "#     print(\"Warning: Could not find encoder layers. Check encoder structure.\")\n",
    "\n",
    "# # Unfreeze last N layers of decoder\n",
    "# if hasattr(model.decoder, \"model\") and hasattr(model.decoder.model, \"decoder\"):\n",
    "#     decoder_layers = model.decoder.model.decoder.layers\n",
    "#     for layer in decoder_layers[-N:]:\n",
    "#         for param in layer.parameters():\n",
    "#             param.requires_grad = True\n",
    "# else:\n",
    "#     print(\"Warning: Could not find decoder layers. Check decoder structure.\")\n",
    "\n",
    "# # Print parameter summary\n",
    "# total_params = sum(p.numel() for p in model.parameters())\n",
    "# trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "# print(f\"Total Parameters: {total_params:,}\")\n",
    "# print(f\"Trainable Parameters (last {N} layers only): {trainable_params:,}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2eIXdQx_8VhA"
   },
   "source": [
    "## Model Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sNNT1XS_CMgl"
   },
   "outputs": [],
   "source": [
    "# Set special tokens used for creating the decoder_input_ids from the labels.\n",
    "model.config.decoder_start_token_id = processor.tokenizer.cls_token_id\n",
    "model.config.pad_token_id = processor.tokenizer.pad_token_id\n",
    "# Set Correct vocab size.\n",
    "model.config.vocab_size = model.config.decoder.vocab_size\n",
    "model.config.eos_token_id = processor.tokenizer.sep_token_id\n",
    "\n",
    "# model.config.max_length = 20\n",
    "# model.config.early_stopping = True\n",
    "# model.config.no_repeat_ngram_size = 3\n",
    "# model.config.length_penalty = 2.0\n",
    "# model.config.num_beams = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K7ixQ5we8VhA"
   },
   "source": [
    "We use the AdamW optimizer here with a weight decay of 0.0005."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m2YsrOv08VhB"
   },
   "outputs": [],
   "source": [
    "optimizer = optim.AdamW(\n",
    "    model.parameters(), lr=TrainingConfig.LEARNING_RATE, weight_decay=0.0005\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5ZDHkmvG8VhB"
   },
   "source": [
    "## Evaluation Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yoXD3_P10DD4"
   },
   "outputs": [],
   "source": [
    "cer_metric = evaluate.load('cer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y36AcnvP0OZw"
   },
   "outputs": [],
   "source": [
    "def compute_cer(pred):\n",
    "    labels_ids = pred.label_ids\n",
    "    pred_ids = pred.predictions\n",
    "\n",
    "    pred_str = processor.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    labels_ids[labels_ids == -100] = processor.tokenizer.pad_token_id\n",
    "    label_str = processor.batch_decode(labels_ids, skip_special_tokens=True)\n",
    "\n",
    "    cer = cer_metric.compute(predictions=pred_str, references=label_str)\n",
    "\n",
    "    return {\"cer\": cer}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mCl0xMRM8VhD"
   },
   "source": [
    "## Training and Validation Loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "32IAm35yyAaB",
    "outputId": "f1fbdbfb-6537-4817-fd39-40ba1f8f00a5"
   },
   "outputs": [],
   "source": [
    "training_args = Seq2SeqTrainingArguments(\n",
    "    predict_with_generate=True,\n",
    "    evaluation_strategy='epoch',\n",
    "    per_device_train_batch_size=TrainingConfig.BATCH_SIZE,\n",
    "    per_device_eval_batch_size=TrainingConfig.BATCH_SIZE,\n",
    "    fp16=True,\n",
    "    output_dir='seq2seq_model_printed/',\n",
    "    logging_strategy='epoch',\n",
    "    save_strategy='epoch',\n",
    "    save_total_limit=5,\n",
    "    report_to='tensorboard',\n",
    "    num_train_epochs=TrainingConfig.EPOCHS\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Evs7yJFcyAaB",
    "outputId": "fca63be4-1893-42e5-f5b7-fe053d594d95"
   },
   "outputs": [],
   "source": [
    "# Initialize trainer.\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    tokenizer=processor.feature_extractor,\n",
    "    args=training_args,\n",
    "    compute_metrics=compute_cer,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=valid_dataset,\n",
    "    data_collator=default_data_collator\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "siJc9cabyAaB"
   },
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eu7LEu3dyAaB",
    "outputId": "1f0325d5-8999-4f81-b758-12150308e1af"
   },
   "outputs": [],
   "source": [
    "res = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IcOIZOjHyAaB",
    "outputId": "da9bb3a1-e989-4480-8db9-7416dabaed56"
   },
   "outputs": [],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5Wf6UWnVyAaB",
    "outputId": "f6b01692-279e-479f-9598-ca9e234352be"
   },
   "outputs": [],
   "source": [
    "res.global_step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qGNhmVKr8VhH"
   },
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PA4IAx-W8VhI"
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7jWAlA188VhI"
   },
   "outputs": [],
   "source": [
    "processor = TrOCRProcessor.from_pretrained(ModelConfig.MODEL_NAME)\n",
    "# trained_model = VisionEncoderDecoderModel.from_pretrained('seq2seq_model_printed/checkpoint-'+str(res.global_step)).to(device)\n",
    "trained_model = VisionEncoderDecoderModel.from_pretrained('seq2seq_model_printed/checkpoint-'+str(res.global_step))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X6Bm3j3gyAaB",
    "outputId": "1655bc39-3ea8-458a-acde-1f3ca0feba0a"
   },
   "outputs": [],
   "source": [
    "trained_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0-QCDbEhztZz"
   },
   "source": [
    "## Saving Model checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dfjTkBlEyAaB"
   },
   "outputs": [],
   "source": [
    "# prompt: give code to zip folder\n",
    "\n",
    "import zipfile\n",
    "\n",
    "def zip_folder(folder_path, zip_path):\n",
    "  \"\"\"Zips a folder.\n",
    "\n",
    "  Args:\n",
    "      folder_path: Path to the folder to zip.\n",
    "      zip_path: Path to the output zip file.\n",
    "  \"\"\"\n",
    "  with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
    "    for root, _, files in os.walk(folder_path):\n",
    "      for file in files:\n",
    "        zipf.write(os.path.join(root, file),\n",
    "                   os.path.relpath(os.path.join(root, file),\n",
    "                                   os.path.join(folder_path, '..')))\n",
    "\n",
    "# Example usage:\n",
    "zip_folder('/content/seq2seq_model_printed/checkpoint-15625', '/content/checkpoint-15625.zip')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ob8oHUKoyAaB"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ogPXdp1lyAaB"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
